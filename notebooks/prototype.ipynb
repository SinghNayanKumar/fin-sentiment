{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d75e14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9579b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'TheFinAI/fiqa-sentiment-classification' from Hugging Face Hub...\n",
      "Dataset loaded. Splits: ['train', 'test', 'valid']\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the dataset from Hugging Face Hub\n",
    "# The 'fiqa-sentiment-classification' dataset has 'train', 'validation', and 'test' splits.\n",
    "print(\"Loading dataset 'TheFinAI/fiqa-sentiment-classification' from Hugging Face Hub...\")\n",
    "ds = load_dataset(\"TheFinAI/fiqa-sentiment-classification\")\n",
    "\n",
    "\n",
    "print(f\"Dataset loaded. Splits: {list(ds.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4de196ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found and mapped: {'headline': 0, 'post': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from 'type' (an string) to an integer ID\n",
    "unique_types = ds['train'].unique('type')\n",
    "type2id = {type_name: i for i, type_name in enumerate(unique_types)}\n",
    "id2type = {i: type_name for type_name, i in type2id.items()}\n",
    "print(f\"Document types found and mapped: {type2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "801b7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    \n",
    "    # Adding Aspect in front of Sentence for Aspect based sentiment analysis\n",
    "    texts = [\n",
    "        f\"{aspect} {tokenizer.sep_token} {sentence}\"\n",
    "        for aspect, sentence in zip(examples['aspect'], examples['sentence'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Tokenize the texts. Padding is handled later by the data collator.\n",
    "    tokenized_inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=256,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # For regression, the 'labels' are the float scores from the dataset.\n",
    "    tokenized_inputs[\"labels\"] = examples['score']\n",
    "\n",
    "    # Add the type_id to our processed data\n",
    "    tokenized_inputs[\"type_ids\"] = [type2id[t] for t in examples['type']]\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "609ee99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2eb3f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and formatting dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 822/822 [00:00<00:00, 10804.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Apply the preprocessing function to all splits of the dataset\n",
    "print(\"Tokenizing and formatting dataset...\")\n",
    "columns_to_remove = ['_id', 'sentence', 'target', 'aspect', 'type', 'score']\n",
    "tokenized_datasets = ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    # Remove original text columns after processing. Keep 'score' to be renamed.\n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "\n",
    "\n",
    "# The labels are already named 'labels' in the preprocessing function, so no need to rename.\n",
    "tokenized_datasets.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"labels\", \"type_ids\"])\n",
    "\n",
    "# 4. Create a Data Collator for dynamic padding\n",
    "# This pads each batch to the length of the longest sequence in that batch.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 5. Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"valid\"],\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(\"DataLoaders created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "46586a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101, 15978,   120,   138,  8661, 21506,  1880,   102,  1787, 11508,\n",
       "          3931,  5554,   139,  1616,  3842,  1383,  1106,  2585,  1205,   102]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor(-0.3740),\n",
       " 'type_ids': tensor(0)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d17b215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save the type mapping to the config for use in evaluation\n",
    "config.ID2TYPE = id2type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acffc43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([16, 57]),\n",
       " 'attention_mask': torch.Size([16, 57]),\n",
       " 'labels': torch.Size([16]),\n",
       " 'type_ids': torch.Size([16])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f5878c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "805f08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Nayan\\Projects\\fin-sentiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_inputs = {\n",
    "    'input_ids': batch['input_ids'].to(device),\n",
    "    'attention_mask': batch['attention_mask'].to(device)\n",
    "}\n",
    "labels = batch[\"labels\"].to(device).unsqueeze(1)\n",
    "\n",
    "# Pass model_inputs using **kwargs and labels separately\n",
    "outputs = model(**model_inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03a117f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1856, grad_fn=<MseLossBackward0>) torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81b96afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d735df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad() #prevents grad accumlation\n",
    "        \n",
    "        # The model only accepts arguments it's designed for.\n",
    "        model_inputs = {\n",
    "            'input_ids': batch['input_ids'].to(device),\n",
    "            'attention_mask': batch['attention_mask'].to(device)\n",
    "        }\n",
    "        labels = batch[\"labels\"].to(device).unsqueeze(1)\n",
    "        \n",
    "        # Pass model_inputs using **kwargs and labels separately\n",
    "        outputs = model(**model_inputs, labels=labels)\n",
    "\n",
    "        \n",
    "        loss = outputs.loss  # This is MSELoss by default for regression\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'MSE_loss': loss.item()})\n",
    "        \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d333417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_label_eval(score):\n",
    "    \"\"\"Converts a predicted score to a label based on thresholds in config.\"\"\"\n",
    "    if score > config.POSITIVE_THRESHOLD:\n",
    "        return 2  # Positive\n",
    "    elif score < config.NEGATIVE_THRESHOLD:\n",
    "        return 0  # Negative\n",
    "    else:\n",
    "        return 1  # NeutralD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a3e88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the regression model and provides both overall and per-type \n",
    "    classification metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predicted_scores = []\n",
    "    all_true_scores = []\n",
    "    all_type_ids = []  # Store document type IDs for stratified analysis\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "        for batch in progress_bar:\n",
    "            # As in training, separate the model inputs from our metadata.\n",
    "            model_inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            # Keep labels and type_ids for our own calculations\n",
    "            true_scores = batch[\"labels\"]\n",
    "            type_ids = batch[\"type_ids\"]\n",
    "            \n",
    "            # Call the model with only the arguments it expects\n",
    "            outputs = model(**model_inputs)\n",
    "            \n",
    "            # Squeeze to remove the last dimension: [batch_size, 1] -> [batch_size] \n",
    "            predicted_scores = outputs.logits.squeeze(-1)\n",
    "            \n",
    "            all_predicted_scores.extend(predicted_scores.cpu().numpy()) #moving batch pred to cpu (required by numpy), changing the type to numpy and appenidng to list\n",
    "            all_true_scores.extend(true_scores.cpu().numpy())\n",
    "            all_type_ids.extend(type_ids.cpu().numpy())\n",
    "\n",
    "    # --- 1. Overall Metrics ---\n",
    "    overall_mse = mean_squared_error(all_true_scores, all_predicted_scores)\n",
    "    overall_mae = mean_absolute_error(all_true_scores, all_predicted_scores)\n",
    "    \n",
    "    # Perform post-hoc classification on all data\n",
    "    predicted_labels = [score_to_label_eval(s) for s in all_predicted_scores]\n",
    "    true_labels = [score_to_label_eval(s) for s in all_true_scores]\n",
    "    \n",
    "    overall_class_report = classification_report(\n",
    "        true_labels, predicted_labels,\n",
    "        target_names=[\"Negative\", \"Neutral\", \"Positive\"], output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    # --- 2. Per-Type Metrics ---\n",
    "    per_type_reports = {}\n",
    "    preds_by_type = defaultdict(list)\n",
    "    labels_by_type = defaultdict(list)\n",
    "\n",
    "    # Group predictions and labels by their document type\n",
    "    for i, type_id in enumerate(all_type_ids):\n",
    "        preds_by_type[type_id].append(predicted_labels[i])\n",
    "        labels_by_type[type_id].append(true_labels[i])\n",
    "    \n",
    "    # Generate a separate classification report for each document type\n",
    "    for type_id, type_name in config.ID2TYPE.items():\n",
    "        if type_id in preds_by_type:  # Check if this type exists in the current data split\n",
    "            report = classification_report(\n",
    "                labels_by_type[type_id],\n",
    "                preds_by_type[type_id],\n",
    "                target_names=[\"Negative\", \"Neutral\", \"Positive\"], output_dict=True, zero_division=0\n",
    "            )\n",
    "            per_type_reports[type_name] = report\n",
    "            \n",
    "    # --- 3. Return a comprehensive dictionary of results ---\n",
    "    return {\n",
    "        \"overall_mse\": overall_mse,\n",
    "        \"overall_mae\": overall_mae,\n",
    "        \"overall_report\": overall_class_report,\n",
    "        \"per_type_reports\": per_type_reports\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c35a30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def get_logger(name):\n",
    "    \"\"\"Initializes and returns a logger.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return logging.getLogger(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d11797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "038bd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10903a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the training and evaluation pipeline.\n",
    "    \"\"\"\n",
    "    # For reproducibility\n",
    "    set_seed(config.RANDOM_SEED)\n",
    "\n",
    "    logger.info(\"--- Starting Financial Sentiment Analysis Pipeline ---\")\n",
    "    logger.info(f\"Configuration: Model={config.MODEL_NAME}, Task={config.TASK_TYPE}, Device={config.DEVICE}\")\n",
    "\n",
    "    #transfering model to device- required by pytorch\n",
    "    model.to(config.DEVICE)\n",
    "\n",
    "\n",
    "    # --- 3. Setup Optimizer and Scheduler ---\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE) #initialising with pre-trained parameters\n",
    "    total_steps = len(train_dataloader) * config.NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    print(f\"Number of training steps: {total_steps}\")\n",
    "\n",
    "    # --- 4. Training Loop ---\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    logger.info(f\"--- Starting Training for {config.NUM_EPOCHS} Epochs ---\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        logger.info(f\"Epoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "        \n",
    "        avg_train_loss = train_epoch(model, train_dataloader, optimizer, config.DEVICE, scheduler)\n",
    "        logger.info(f\"Average Training MSE Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        val_results = evaluate(model, valid_dataloader, config.DEVICE)\n",
    "        val_f1 = val_results['overall_report']['weighted avg']['f1-score']\n",
    "        val_mse = val_results['overall_mse']\n",
    "        \n",
    "        logger.info(f\"Validation MSE: {val_mse:.4f} | Validation Weighted F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # Save the best model based on validation F1-score\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            logger.info(f\"New best model found with F1: {best_val_f1:.4f} and saved.\")\n",
    "\n",
    "    # --- 5. Final Evaluation on Test Set ---\n",
    "    logger.info(\"--- Training Finished. Starting Final Evaluation on Test Set ---\")\n",
    "    # Load the best performing model for the final test\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    else:\n",
    "        logger.warning(\"No best model was saved. Evaluating the model from the last epoch.\")\n",
    "    \n",
    "    test_results = evaluate(model, test_dataloader, config.DEVICE)\n",
    "    \n",
    "    # Display results in a clean format\n",
    "    logger.info(\"--- Overall Test Results ---\")\n",
    "    logger.info(f\"Regression MSE: {test_results['overall_mse']:.4f}\")\n",
    "    logger.info(f\"Regression MAE: {test_results['overall_mae']:.4f}\")\n",
    "    \n",
    "    overall_df = pd.DataFrame(test_results['overall_report']).transpose()\n",
    "    logger.info(\"Overall Classification Report:\\n\" + overall_df.to_string())\n",
    "\n",
    "    logger.info(\"\\n--- Per-Type Test Results ---\")\n",
    "    for type_name, report in test_results['per_type_reports'].items():\n",
    "        logger.info(f\"\\n--- Report for Document Type: '{type_name}' ---\")\n",
    "        type_df = pd.DataFrame(report).transpose()\n",
    "        logger.info(\"\\n\" + type_df.to_string())\n",
    "\n",
    "    logger.info(\"--- Pipeline Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9c9c114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 22:55:44 - __main__ - INFO - --- Starting Financial Sentiment Analysis Pipeline ---\n",
      "2025-07-30 22:55:44 - __main__ - INFO - Configuration: Model=bert-base-cased, Task=absa, Device=cpu\n",
      "2025-07-30 22:55:44 - __main__ - INFO - --- Starting Training for 4 Epochs ---\n",
      "2025-07-30 22:55:44 - __main__ - INFO - Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 22:57:36 - __main__ - INFO - Average Training MSE Loss: 0.1683 \n",
      "2025-07-30 22:57:40 - __main__ - INFO - Validation MSE: 0.1756 | Validation Weighted F1: 0.5325\n",
      "2025-07-30 22:57:40 - __main__ - INFO - New best model found with F1: 0.5325 and saved.\n",
      "2025-07-30 22:57:40 - __main__ - INFO - Epoch 2/4\n",
      "Training:   0%|          | 0/52 [00:00<?, ?it/s]c:\\Nayan\\Projects\\fin-sentiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "2025-07-30 22:59:32 - __main__ - INFO - Average Training MSE Loss: 0.0977 \n",
      "2025-07-30 22:59:36 - __main__ - INFO - Validation MSE: 0.0792 | Validation Weighted F1: 0.7416\n",
      "2025-07-30 22:59:36 - __main__ - INFO - New best model found with F1: 0.7416 and saved.\n",
      "2025-07-30 22:59:36 - __main__ - INFO - Epoch 3/4\n",
      "Training:   0%|          | 0/52 [00:00<?, ?it/s]c:\\Nayan\\Projects\\fin-sentiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "2025-07-30 23:01:35 - __main__ - INFO - Average Training MSE Loss: 0.0553 \n",
      "2025-07-30 23:01:39 - __main__ - INFO - Validation MSE: 0.0836 | Validation Weighted F1: 0.7174\n",
      "2025-07-30 23:01:39 - __main__ - INFO - Epoch 4/4\n",
      "Training:   0%|          | 0/52 [00:00<?, ?it/s]c:\\Nayan\\Projects\\fin-sentiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "2025-07-30 23:03:43 - __main__ - INFO - Average Training MSE Loss: 0.0381  \n",
      "2025-07-30 23:03:46 - __main__ - INFO - Validation MSE: 0.0735 | Validation Weighted F1: 0.7417\n",
      "2025-07-30 23:03:46 - __main__ - INFO - New best model found with F1: 0.7417 and saved.\n",
      "2025-07-30 23:03:46 - __main__ - INFO - --- Training Finished. Starting Final Evaluation on Test Set ---\n",
      "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]c:\\Nayan\\Projects\\fin-sentiment\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "2025-07-30 23:03:54 - __main__ - INFO - --- Overall Test Results ---\n",
      "2025-07-30 23:03:54 - __main__ - INFO - Regression MSE: 0.0730\n",
      "2025-07-30 23:03:54 - __main__ - INFO - Regression MAE: 0.1962\n",
      "2025-07-30 23:03:54 - __main__ - INFO - Overall Classification Report:\n",
      "              precision    recall  f1-score     support\n",
      "Negative       0.861111  0.738095  0.794872   84.000000\n",
      "Neutral        0.071429  0.166667  0.100000   12.000000\n",
      "Positive       0.865672  0.840580  0.852941  138.000000\n",
      "accuracy       0.769231  0.769231  0.769231    0.769231\n",
      "macro avg      0.599404  0.581781  0.582604  234.000000\n",
      "weighted avg   0.823304  0.769231  0.793483  234.000000\n",
      "2025-07-30 23:03:54 - __main__ - INFO - \n",
      "--- Per-Type Test Results ---\n",
      "2025-07-30 23:03:54 - __main__ - INFO - \n",
      "--- Report for Document Type: 'headline' ---\n",
      "2025-07-30 23:03:54 - __main__ - INFO - \n",
      "              precision    recall  f1-score    support\n",
      "Negative       0.880000  0.628571  0.733333  35.000000\n",
      "Neutral        0.111111  0.285714  0.160000   7.000000\n",
      "Positive       0.785714  0.771930  0.778761  57.000000\n",
      "accuracy       0.686869  0.686869  0.686869   0.686869\n",
      "macro avg      0.592275  0.562072  0.557365  99.000000\n",
      "weighted avg   0.771348  0.686869  0.718950  99.000000\n",
      "2025-07-30 23:03:54 - __main__ - INFO - \n",
      "--- Report for Document Type: 'post' ---\n",
      "2025-07-30 23:03:54 - __main__ - INFO - \n",
      "              precision    recall  f1-score    support\n",
      "Negative       0.851064  0.816327  0.833333   49.00000\n",
      "Neutral        0.000000  0.000000  0.000000    5.00000\n",
      "Positive       0.923077  0.888889  0.905660   81.00000\n",
      "accuracy       0.829630  0.829630  0.829630    0.82963\n",
      "macro avg      0.591380  0.568405  0.579665  135.00000\n",
      "weighted avg   0.862751  0.829630  0.845865  135.00000\n",
      "2025-07-30 23:03:54 - __main__ - INFO - --- Pipeline Finished ---\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
